[source](https://juejin.im/post/5dff8a26e51d4558105420ed)

大文件上传和断点续传
在线coding的编程题， 思路正确， 不算完全答对。

vue element-ui
node.js

Blob.prototype.slice 方法， 和数组的slice相似， 原文件的某个切片。
http的可并发性， 同时上传多个切片， 原本传一个大文件， 变成同时传多个小的文件切片，大大减少上传时间， 切片记录顺序。

前端数量信息 接受到这个数量后自动合并， 额外发一个请求主动通知服务器进行切片的合并。
readStream, writeStream 

# 切片

文件上传是开发中的难点，面试官在考察es6文件对象、ajax异步上传、后台文件存储、流操作等全栈技能的同时，提升难度点到大文件和断点续传，通过这个主题，就可以较好的考察面试者全面解决问题的能力和技术细节。移动时代图片成为社交的主流，短视屏铁定是大文件，所以在上岗后，这个考题的知识点是必须掌握清楚的，所以这也是一道非常实在的好考题。 


JS都没有比较好的可以直接处理二进制的方法   而Blob的存在，允许我们可以通过JS直接操作二进制数据。
一个不可变的, 原始数据的类似文件对象
File 基于Blob 用户系统上的文件

- 传输过程中难免出现切片丢失的情况， 网速， 服务器超时， 如何避免丢失呢？ 服务器接收到文件读取文件的MD5值，然后跟前端传递的MD5进行比对， 相同则文件数据未丢失，不相同证明文件信息丢失。

- 无论是前端还是后端， 都必须要生成文件和切片的 hash， 之前我们使用文件名 + 切片下标作为切片 hash，这样做文件名一旦修改就失去了效果，而事实上只要文件内容不变，hash 就不应该变化，所以正确的做法是根据文件内容生成 hash，所以我们修改一下 hash 的生成规则
spark-md5，它可以根据文件内容计算出文件的 hash 值，另外考虑到如果上传一个超大文件，读取文件内容计算 hash 是非常耗费时间的，并且会引起 UI 的阻塞，导致页面假死状态，所以我们使用 web-worker 在 worker 线程计算 hash，这样用户仍可以在主界面正常的交互

由于实例化 web-worker 时，参数是一个 js 文件路径且不能跨域，所以我们单独创建一个 hash.js 文件放在 public 目录下，另外在 worker 中也是不允许访问 dom 的，但它提供了importScripts 函数用于导入外部脚本，通过它导入 spark-md5

self.importScripts("/spark-md5.min.js");